Create a new service account with the name pvviewer. Grant this Service account access to list all PersistentVolumes in the cluster by creating an appropriate cluster role called pvviewer-role and ClusterRoleBinding called pvviewer-role-binding.
Next, create a pod called pvviewer with the image: redis and serviceAccount: pvviewer in the default namespace.


ServiceAccount: pvviewer
ClusterRole: pvviewer-role
ClusterRoleBinding: pvviewer-role-binding
Pod: pvviewer
Pod configured to use ServiceAccount pvviewer ?

solution:
kubectl create serviceaccount pvviewer
k create clusterrole pvviewer-role --verb=list --resource=PersistentVolumes
kubectl create clusterrolebinding pvviewer-role-binding --clusterrole=pvviewer-role --serviceaccount=default:pvviewer


cat pvviewer-pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: pvviewer
spec:
  containers:
  - image: redis
    name: redis
  serviceAccountName: pvviewer


______________________________

List the InternalIP of all nodes of the cluster. Save the result to a file /root/CKA/node_ips.
Answer should be in the format: InternalIP of controlplane<space>InternalIP of node01 (in a single line)

solution:
kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}' > /root/CKA/node_ips

link:https://kubernetes.io/docs/reference/kubectl/cheatsheet/#viewing-and-finding-resources
______________________________

Create a pod called multi-pod with two containers.
Container 1, name: alpha, image: nginx
Container 2: name: beta, image: busybox, command: sleep 4800

Environment Variables:
container 1:
name: alpha

Container 2:
name: beta

solution:
apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
spec:
  containers:
  - name: alpha
    image: nginx
    env:
    - name: name
      value: alpha
  - name: beta
    image: busybox
    command: ["sleep", "4800"]
    env:
    - name: name         
      value: beta

______________________________

Create a Pod called non-root-pod , image: redis:alpine
runAsUser: 1000
fsGroup: 2000

solution:
apiVersion: v1
kind: Pod
metadata:
  name: non-root-pod
spec:
  containers:
  - image: redis:alpine
    name: redis
  securityContext:
    runAsUser: 1000
    fsGroup: 2000
______________________________

We have deployed a new pod called np-test-1 and a service called np-test-service. Incoming connections to this service are not working. Troubleshoot and fix it.
Create NetworkPolicy, by the name ingress-to-nptest that allows incoming connections to the service over port 80.

Important: Don't delete any current objects deployed.
Important: Don't Alter Existing Objects!
NetworkPolicy: Applied to All sources (Incoming traffic from all pods)?
NetWorkPolicy: Correct Port?
NetWorkPolicy: Applied to correct Pod?

solution:
kind: NetworkPolicy
metadata:
  name: ingress-to-nptest
  namespace: default
spec:
  podSelector:
    matchLabels:
      run: np-test-1
  policyTypes:
    - Ingress
  ingress:
  - ports:
        - protocol: TCP
          port: 80
______________________________

Taint the worker node node01 to be Unschedulable. Once done, create a pod called dev-redis, image redis:alpine, to ensure workloads are not scheduled to this worker node. Finally, create a new pod called prod-redis and image: redis:alpine with toleration to be scheduled on node01.


key: env_type, value: production, operator: Equal and effect: NoSchedule
Key = env_type
Value = production
Effect = NoSchedule
pod 'dev-redis' (no tolerations) is not scheduled on node01?
Create a pod 'prod-redis' to run on node01


solution:
kubectl taint nodes node01 env_type=production:NoSchedule
k run dev-redis --image=redis:alpine --> this pod will not be scheduled on node01 


apiVersion: v1
kind: Pod
metadata:
  name: prod-redis
  labels:
    env: test
spec:
  containers:
  - name: prod-redis
    image: redis:alpine
    imagePullPolicy: IfNotPresent
  tolerations:
  - key: "env_type"
    operator: "Equal"
    value: "production"
    effect: "NoSchedule"

the above pod will be scheduled on node01 as its toleration is matching the node01's taint 
______________________________

Create a pod called hr-pod in hr namespace belonging to the production environment and frontend tier .
image: redis:alpine

Use appropriate labels and create all the required objects if it does not exist in the system already.
hr-pod labeled with environment production?
hr-pod labeled with tier frontend?


solution:
apiVersion: v1
kind: Pod
metadata:
  name: hr-pod
  namespace: hr
  labels:
    environment: production
    tier: frontend
spec:
  containers:
  - name: redis
    image: redis:alpine
    ports:
    - containerPort: 80

______________________________

A kubeconfig file called super.kubeconfig has been created under /root/CKA. There is something wrong with the configuration. Troubleshoot and fix it.


solution:
port issue
______________________________

We have created a new deployment called nginx-deploy. scale the deployment to 3 replicas. Has the replica's increased? Troubleshoot the issue and fix it.


solution:
Remember when replicaset is not scaling pods there is always a issue with this pod named "kube-controller-manager-controlplane" in namespace: kube-system

k describe pod kube-contro1ler-manager-controlplane -n kube-system

Since its a static pod: the file /etc/kubernetes/manifests/kube-controller-manager.yaml has issues
sed -i 's/contro1ler/controller/g' /etc/kubernetes/manifests/kube-controller-manager.yaml
k scale --replicas=3 deployment/nginx-deploy
k get pods --> pods will be deployed




















